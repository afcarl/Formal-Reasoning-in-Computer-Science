\documentclass[10pt]{article}
\usepackage{amsmath} 
\usepackage{ifpdf} 
\pagestyle{headings}
\textheight    8.4in
\textwidth      6.2in
\topmargin        0in
\oddsidemargin  0in
\evensidemargin 0in

\newcounter{set}
\def\theset{\arabic{set}}
\def\theequation{\theset.\arabic{equation}}

\title{Probability and Computing Notes and Questions} 
 \markboth{Chp6}{}
\author{Angjoo Kanazawa}

\date{\today}
\setcounter{set}{6}


\begin{document}
\maketitle \thispagestyle{empty}

\section{Chapter 6 The Probabilistic Method}
\label{sec:chap6}


\subsection{Notes}
\begin{itemize}
\item To prove teh existence of an object with specific properties,
  construct an approximate probability space of objects $\mathit{S}$,
  and show that the probability that an object (in $\mathit{S}$ with
  the specific proprties) is selected is $> 0$. Strictly.
\item 
\end{itemize}

\subsection{Exercises}

\textbf{6.1} 
\label{Q6.1}
Consider an instance of SAT with $m$ clauses, where every clause has
exactly $k$ literals.
\begin{enumerate}
\item[(a)]  Give a Las Vegas algorithm that finds an
assignment that satisfies at least $m(1-2^{-k})$ clauses, anayze its
expected running time:\\

The first part is straight from the book (6.2.2). Note:
$$P(\text{a clause is satisfied}) = 1 - P(\text{all literals are
  false}) = 1 - 2^{-k}$$
Let $i= 1\dots m$, $X_i = 1$ if $i$th clause is satisfied, $X_i=0$
otherwise. 
Let $X = \sum_{i}^m X_i$, the total number of satisfied clauses.
So we get $$E(X) = \sum_i^m X_iP(X_i=1) = m(1-2^{-k}) = \mu$$

Now given this, the LV algorithm goes like this:

Repeat until $X \ge \mu$:
\begin{itemize}
\item assign values to all boolean variables independently and uniformly.
\item Check the value of $X$.
\end{itemize}

What is the expected number of runs until LV finishes? We're done
when $X \ge \mu$. Note that $X$
has a binomial distribution*. Let $Y$ be the number of runs needed for
the LV to terminate, i.e. number of trials before the first sucess. So
$Y$ has a geometric distribution and let $\hat{p}$ be the probability
of success at each trial i.e. $\hat{p} = P(X\ge \mu)$. Then, $E(Y)$, what we
want, is*
\begin{equation}
  \label{eq:1}
E(Y) = \sum_{I=1}^\infty i\hat{p}(1-\hat{p})^{i-1} = 1/p
\end{equation}


So for each run, what is $\hat{p} = P(X\ge \mu)$? Using $p
= P(\text{a clause is satisfied}) = 1 - 2^{-k}$:
\begin{align*}
  P(X\ge \mu) &= 1 - P(X < \mu)\\
&= 1 - [ P(X=0) + P(X=1) + \cdots + P(X = \mu -1)]\\
&= 1 - [(1-p)^m + {m \choose 1}p(1-p)^{m-1} + \cdots + {m
 \choose \mu - 1} p^{\mu-1}(1-p)^{m -(\mu -1)}]\\
&= 1 - \sum_{i=0}^{\mu-1}{m \choose i}p^i(1-p)^{m-i}\\
\end{align*}

Now recall $\sum_i^m{m \choose i} = 2^m$, so $\sum_i^{\mu -1} {m
  \choose i} \le \frac{2^m}{2} = 2^{m-1}$. 

Also, notice that here $p^i(1-p)^{m-i} = (1-2^{-k})^i(1-2^{-k})^{m-i}
= (1-2^{-k})^m$.

So we can continue the above inequalitywith:
\begin{align*}
  P(X\ge \mu) &= 1 - \sum_{i=0}^{\mu-1}{m \choose i}p^i(1-p)^{m-i}\\
  &\ge 1 -[ 2^{m-1} (1-2^{-k})^m] = 1-2^{-k}
\end{align*}
As an example, with $k=1$ $\hat{p}$ is  $1/2$.
Using \ref{eq:1}, we get $E[Y] = 2$.

.. If $E[Y] = \frac{1}{1-2^{-k}}$, the bigger the $k$, the
faster the algorithm.. But that makes sense.

Can one always say $P(X\ge \mu) \ge 1/2$?

\item[(b)] Give a derandomization of the randomized algorithm using
  the method of conditional expectations:\\
This I also just followed the book. Maybe too closely. 

We know setting variables independently and uniformly gives us $E(X)
\ge m(1-2^{-k})$. Now set the boolean variables $x_1, x_2, \dots$  up
to $r$ deterministically one at a time.

Consider the expected total \# of satisfied clauses if the remaining
boolean variables are selected independently and uniformly. Write this
as $E(X|x_1, x_2, \dots, x_r)$. We want a away o set the next variable
s.t.
\begin{equation}
  \label{eq:2}E(X|x_1, \dots, x_r) \le E(X|x_1, \dots, x_r, x_{r+1})  
\end{equation}
Inductively, the base case is $E(X|x_1) = E(X)$. Now, consider setting
$x_{r+1}$ randomly to true or false. Each has probability $1/2$. So
$E(X|x_1,\dots,x_r) = \frac{1}{2}E(X|x_1,\dots,x_{r+1} = 1) +
\frac{1}{2}E(X|x_1,\dots, x_{r+1} = 0)$
From this we can deduce $$\max(E(X|x_1,\dots,x_r,x_{r+1} = 1),
E(X|x_1,\dots,x_r,x_{r+1} = 0)) \ge E(X|x_1,\dots, x_r)$$ 

So we just have to chose the assignment that increases the
conditional expectation the most.. we only have two options $x_{r+1}$
is T or F, so look at clauses that contain the $x_{r+1}$ variable
twice and see how the expectation changes based on the assignment and
take the better one? Something like that.

\end{enumerate}

\section{Chapter 7 Markov Chains and Random Walks}
\label{sec:chap7}

\subsection{Markov Chains: Definitions and Representations}
\label{sec:7.1}
\begin{itemize}
\item \textbf{Stochastic process} $\mathbf{X} = \{X(t): t\in T\}$ is a collection of random variables. Often $t$ represents time, where $\mathbf{X}$ models the value of a r.v. $X$ that changes over time.
\item $X(t)$, or $X_t$ is the \emph{state} of the process at time $t$.
\item If $X_t$ assumes values from a countably infinite set, $\mathbf{X}$ is a \textbf{discrete space process}. If $X_t$ assumes values from a finite set, then $\mathbf{X}$ is a \textbf{finite process}. If $T$ is a countably finite set, then $\mathbf{X}$ is a \textbf{discrete \emph{time} process}.
\item Here we focus on a process where $X_t$ depends on $X_{t-1}$, but no other states.
\item \textbf{Definition:} a \emph{discrete time stochastic process} $X_0, X_1, \dots$ is a Markov chain if 
  \begin{align*}
Pr(X_t = a_t | X_{t-1} = a_{t-1}, X_{t-2} = a_{t-2}, \dots, X_0 = a_0) &= P(X_t = a_t | X_{t-1} a_{t-1})\\
&= P_{a_{t-1}, a_t\cdot}
  \end{align*} What we said on the point above. $X_t$ depends on $X_{t-1}$ but not on any other past. How we got to $X_{t-1}$ is irrelevant, because all the dependency of $X_t$ is captured by $X_{t-1}$, and this is the \emph{Markov property}.
\item the \emph{transition probability} $P_{i,j} = P(X_t = j | X_{t-1} = i)$ is the probability that the process moves from $i$ to $j$ in \emph{one step}. The markov property implies that the Markov chain is uniquely defined by the one step \emph{transition matrix}, $\mathbf{P}$, made up of $P_{i,j}$s. $\forall i, \sum_{j\ge 0} P_{i,j} = 1$.
\item The transition matrix is used to computing the distribution of future states of the process. if $p_i(t)$ denotes hte probability that the process is at state \textbf{$i$} at time $t$, and $\bar{p}(t)$ be the vector of the distribution of the state of the chain at time $t$ (future), summing over al possible states at time $t-1$ we have $$\bar{p}(t) = \bar{p}(t-1)\mathbf{P}$$
\item for any $n$, the \emph{n-step transition probability} $P^n_{i,j} = P(X_{t+n} = j | X_t = i)$ is the probability that the chain moves from state $i$ to state $j$ in exactly $n$ steps.
\item using the first transition from $i$, we get $$P^n{i,j} = \sum_{k\ge 0} P_{i, k}P^{n-1}_{k,j}$$
\item So $P^{(n)}$ is the n-step transition matrix, where the entry in the $i$th row and $j$th col is $P^n_{i,j}$. Then from the above we have $\mathbf{P^{(n)}} = \mathbf{P}\cdot \mathbf{P^{(n-1)}}$. Keep on doing this, we see that $\mathbf{P^{(n)}}$ is just $\mathbf{P^n}$.
\item So, $\forall t\ge 0 \text{ and } n \ge 1$, $$\bar{p}(t+n) = \bar{p}(t)\mathbf{P}^n$$

\item People represent a Markov chain by a directed, weighted graph $D=(V, E, w)$, where $V$ is the set of states of the chain. $\exists$ an edge $(i,j)\in E$ iff $P_{i,j} > 0$, where the weight, $w(i,j)$ of that edge is $P_{i,j}$. Self-loops are okay. For each $i$ (a state), $\sum_{(i,j) \in E} w(i,j) = 1$. A sequence of visited states is a directed path on $D$.
\item Example, to calculate the probability of going from state 0 to state 3 \emph{in 3 steps} is the $(0,3)$ entry in $\mathbf{P}^3$. The probability of ending in state 3 after 3 steps where the beginning state if chosen uniformly at random from $n$ states is $(1/n, 1/n, \dots, 1/n)\mathbf{P}^3$. where the $|V|$ long row vector $(1/n, 1/n, \dots, 1/n)$ is the $\bar{p}(0)$ here. (Remember it's right multiplication). That was just $\bar{p}(0+3) = \bar{p}(0)\mathbf{P}^n$.
\end{itemize}





\end{document}
